---
layout: post
title:  "Lecture 0 of Harvard's CS50x"
date:   2024-07-08
categories: [computer science]
---
<div class="note">
    <p class="note-title">Use of AI</p>
    <p>This post used to live on <a href="https://reneelearnsthings.blogspot.com/">Blogger</a> and was written in <code>HTML</code>. I migrated it to <a href="https://www.reneedesporte.com/2025/10/22/setting-up-my-website-with-github-pages-and-jekyll/">my website</a> (written in <code>markdown</code> for Jekyll) partially using ChatGPT.</p>
</div>

# Introduction
I'm just watching the videos from [this course](https://cs50.harvard.edu/x/) to get a more formal understanding of some fundamental concepts in CS, such as efficiency and [Big O](https://en.wikipedia.org/wiki/Big_O_notation) notation. I won't be doing the problem sets, though they're free to access of Harvard edX website!
# Notes
- Computers deal with information in **_binary_** format.
- Each switch within a computer is either on (1) or off (0) and is called a **_bit_**.
- The physical switches within these computers are called **_transistors_**.
- The switches combine in a **_base-2_** format to form larger numbers.

### Base-2 Representation of Numbers
- In base-10, numbers are represented as powers of 10, e.g., $12 = 1 \times 10^{1} + 2 \times 10^{0}$.
- Similarly in base-2, numbers are represented as powers of 2, e.g., $11 = 1 \times (2^{1}) + 1 \times (2^{0}) = 3$ (in base-10).
- The largest number you can represent with N binary digits (or **_bits_**) is $2^N - 1$, since we include 0.
- In general, if you have N bits, you can **_permute_** them in $2^N$ ways.
- Bits can also be used to represent letters (and in fact they must be, if we ever want to send information in the form of letters and words)
- In **_ASCII_** (American Standard Code for Information Interchange), the letter A is represented by the decimal digits 65, or the binary digits 01000001.
- ASCII uses 8 bits to represents numbers, letter, and symbols &mdash; plenty of permutations (256) for English, but not enough for a language like Chinese.
- **_Unicode_** uses 32 bits to represent characters and symbols, and this regime has largely replaced ASCII, since unicode can represent _billions_ of symbols.
- Base-2 representations can get _long_ at this point, e.g., 10011001010000010111001100111111.
- Even the decimal representations can be too long. So a new representation, **_hexidecimal_**, can be used. It'll look something like this: U+1F602.
- Colors, music, numbers, letter, photos and videos can all be represented in computers.
- In fact, anything can be represented digitally, with bits.

### Algorithms
- An **_algorithm_** is a way, a formula of steps, on how to solve a problem.
- At the very least, our goal should be an algorithm which solves our problem _correctly_.
- But we also aim for _efficiency_.

# Conclusion
Not too much new info in the first lecture, but the lecturer is interesting and engaging!

<div class="note">
    <p class="note-title">Edit</p>
    <p>I've decided not to proceed with future lectures, as they seem a bit too basic for my level of coding experience. But I may refer to to the SQL lecture later, since I have very little experience with that!</p>
</div>
